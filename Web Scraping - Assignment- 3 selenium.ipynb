{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7158cf9",
   "metadata": {},
   "source": [
    "# 1.Write a python program which searches all the product under a particular product from www.amazon.in. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1321e6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "option = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver=webdriver.Chrome()\n",
    "driver.get('https://www.amazon.in/')\n",
    "driver.find_element(By.ID,\"twotabsearchtextbox\").send_keys(\"guitar\")\n",
    "driver.find_element(By.ID,\"nav-search-submit-button\").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f368952e",
   "metadata": {},
   "source": [
    "# 2.In the above question, now scrape the following details of each product listed in first 3 pages of your search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0cbc5e",
   "metadata": {},
   "source": [
    "# 3. Write a python program to access the search bar and search button on images.google.com and scrape 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6cdcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "option = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://images.google.com/')\n",
    "driver.find_element(By.NAME,\"q\").send_keys('fruits')\n",
    "driver.find_element(By.NAME,\"q\").clear()\n",
    "driver.find_element(By.NAME,\"q\").send_keys('cars')\n",
    "driver.find_element(By.NAME,\"q\").clear()\n",
    "driver.find_element(By.NAME,\"q\").send_keys('machine learning')\n",
    "driver.find_element(By.NAME,\"q\").clear()\n",
    "driver.find_element(By.NAME,\"q\").send_keys('guitar')\n",
    "driver.find_element(By.NAME,\"q\").clear()\n",
    "driver.find_element(By.NAME,\"q\").send_keys('cakes')\n",
    "driver.find_element(By.NAME,\"q\").clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7125aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "driver=webdriver.Chrome()\n",
    "driver.get('https://images.google.com')\n",
    "keywords=['fruits','cars','machine learning','guitar','cakes']\n",
    "for keyword in keywords:\n",
    "    search_bar=driver.find_element(By.NAME,'q')\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(keywords)\n",
    "    search_buttons=driver.find_element(By.CSS_SELECTOR,('button[ aria-label=\"Google Search\"]'))\n",
    "    search_button.click()\n",
    "    time.sleep(2)\n",
    "    driver.execute_script('window.scrollTo(0,document.body.scrollHeight);')\n",
    "    time.sleep()\n",
    "    images=driver.find_elements(By.CSS_SELECTOR,('img.rg_i'))\n",
    "    for i in range(10):\n",
    "        image_url=images[i].get_attribute('src')\n",
    "        print(f\"Scraped image{i+1}for keyword'{keyword}':\n",
    "             {image_url}\")\n",
    "        driver.execute_script('window.history.go(-1)')\n",
    "              time.sleep(2)\n",
    "        driver.quit()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc49eb7d",
   "metadata": {},
   "source": [
    "# 4.Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a056e111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "def scrape_smartphones():\n",
    "    url='https://www.flipkart.com/search?q=smartphone'\n",
    "    response=requests.get(url)\n",
    "    soup=BeautifulSoup(response.content,'html.parser')\n",
    "    smartphones=[]\n",
    "    results=soup.find_all('div',{'class':'_1AtVbE'})\n",
    "    for result in results:\n",
    "        details={}\n",
    "    details['Brand name']=result.find('div',{'class':'_4rR01T'}).text\n",
    "    details['smartphone name']=result.find('a',{'class':'_2WKVRV'}).text\n",
    "    details['RAM']=result.find('u1',{'class':'_1xgFaf'}).text\n",
    "    details['ROM']=result.find('u1',{'class':'_1xgFaf'}).find_all('li')[2].text\n",
    "    details['display size']=result.find('u1',{'class':'_1xgFaf'}).find_all('li')[4].text\n",
    "    details['Battery Capacity']=result.find('u1',{'class':'_1xgFaf'}).find_all('li')[5].text\n",
    "    details['price']=result.find('div',{'class':'_30jeq3 _1_WHN1'}).text\n",
    "    details['product url']='https://www.flipkart.com'+result.find('a',{'class':'IRpwTa'})['href']\n",
    "    smartphones.append(details)\n",
    "    return smartphones\n",
    "smartphones=scrape_smartphones()\n",
    "df=pd.DataFrame(smartphones)\n",
    "df.fillna('-',inplace=True)\n",
    "df.to_csv('smartphones.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcac87b1",
   "metadata": {},
   "source": [
    "# 5.Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614bec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "def get_coordinates(city):\n",
    "    formatted_city=city.replace(\"\",\"+\")\n",
    "    url=f\"https://www.google.com/maps/search/{formatted_city}\"\n",
    "    response=requests.get(url)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    coordinates_element=soup.find('meta',itemprop='image')\n",
    "    coordinates=coordinates_element['content'].split(\";\")[1].strip().split(',')\n",
    "    longitude as a tuple\n",
    "    return float(coordinates[0]),float(coordinates[1])\n",
    "city=input('Enter a city name:')\n",
    "latitude,longitude=get_coordinates(city)\n",
    "print(f'The coordinates of {city}are:Latitude:{latitude},Longitude:{longitude}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1770cbb4",
   "metadata": {},
   "source": [
    "# 6.Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae9e8c9",
   "metadata": {},
   "source": [
    "# 7.Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43179980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url='https://www.forbes.com'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.content,\"html.parser\")\n",
    "table=soup.find('table',class_='table')\n",
    "rows=soup.find_all('tr')\n",
    "for row in rows:\n",
    "    columns=row.find_all('td')\n",
    "    rank=columns[0].text.strip()\n",
    "    name=columns[1].text.strip()\n",
    "    net_worth=columns[2].text.strip()\n",
    "    age=columns[3].text.strip()\n",
    "    citizenship=columns[4].text.strip()\n",
    "    sourse=columns[5].text.strip()\n",
    "    industry=columns[6].text.strip()\n",
    "    print('Rank:',rank)\n",
    "    print('Name:',name)\n",
    "    print('Net worth',net_worth)\n",
    "    print('Age:',age)\n",
    "    print('Citizenship:',citizenship)\n",
    "    print('Sourse:',sourse)\n",
    "    print('industry:',industry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2188cc4",
   "metadata": {},
   "source": [
    "# 8.Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e0c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "driver=webdriver.Chrome()\n",
    "url='https://www.youtube.com/watch?v=kffacxfA7G4'\n",
    "driver.get(url)\n",
    "scroll_pause_time=2\n",
    "scrolls=500\n",
    "for _ in range(scrolls):\n",
    "    driver.execute_script('window.scrollTo(0,document.documentElement.scrollHeight);')\n",
    "    time.sleep(scroll_pause_time)\n",
    "    comments=driver.find_elements(By.ID,\"content\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6425b5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
